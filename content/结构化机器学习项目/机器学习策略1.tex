\documentclass[../../main.tex]{subfiles}
\begin{document}
\chapter{机器学习策略1}

\section{为什么是ML策略？}
% Why ML Strategy?
改进的方法有很多，我们有可能浪费大量时间在一条错误的改进路线上。想要找准改进的方向，使一个机器学习系统更快更有效地工作，就需要学习一些在构建机器学习系统时常用到的策略。

\section{正交化}
% Orthogonalization
我们的调整可能影响模型的多方面，这时优化是十分低效的。我们需要\textbf{正交化（Orthogonalization）}，即保证每次调整只会影响模型某一方面的性能，而对其他功能没有影响。这种方法有助于更快更有效地进行机器学习模型的调试和优化。

比如在机器学习（监督学习）系统中，可以划分四个“功能”：
\begin{enumerate}
    \item 建立的模型在训练集上表现良好；
    \item 建立的模型在验证集上表现良好；
    \item 建立的模型在测试集上表现良好；
    \item 建立的模型在实际应用中表现良好。
\end{enumerate}
那么符合正交化的调整有：
\begin{itemize}
    \item 如果模型在训练集上表现不好，可以尝试训练更大的神经网络或者换一种更好的优化算法（例如 Adam）；
    \item 如果模型在验证集上表现不好，可以进行正则化处理或者加入更多训练数据；
    \item 如果模型在测试集上表现不好，可以尝试使用更大的验证集进行验证；
    \item 如果模型在实际应用中表现不好，可能是因为测试集没有设置正确或者成本函数评估指标有误，需要改变测试集或成本函数。
\end{itemize}
而不符合正交化的手段典型例子是Early Stopping，虽然可以改善验证集的拟合表现，但是对训练集的拟合就不太好。虽然也可以使用，但是用其他正交化控制手段来进行优化会更简单有效。

\section{单值评估指标}
% Single number evaluation metric
构建机器学习系统时，通过设置一个量化的\textbf{单值评估指标（single-number evaluation metric）}，可以使我们根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。

比如说对于二分类，有两个评价指标：\textbf{精确率（Precision）}和\textbf{召回率（Recall）}，如果A算法在精确率上表现比较好，而B算法在召回率上表现比较好，那么就很难去快速地二中选一或者十中选一，此时就需要一个新的能够结合精确率和召回率的评估指，比如说\textbf{F1 Score}。F1 Score其实就是精准率和召回率的调和平均数（Harmonic Mean），比单纯的平均数效果要好。\[
    F1 = \frac{2}{\frac{1}{P}+\frac{1}{R}} = \frac{2PR}{P+R}
\]

\begin{remark}[精确率和召回率]

    对于预测结果而言，预测为正就有两种可能：一种就是把正类预测为正类（TP），另一种就是把负类预测为正类（FP），那么\textbf{精确率（Precision）}就是：\[
        P = \frac{TP}{TP + FP}
    \]
    其意义是预测正确可能性，即“精确率”。
    对于原来的样本集而言，也有两种可能：一种是把原来的正类预测成正类（TP），另一种就是把原来的正类预测为负类（FN），那么\textbf{召回率（Recall）}就是：\[
        R = \frac{TP}{TP + FN}
    \]
    其意义是样本中的正例有多少被预测正确了。

    在信息检索领域，精确率和召回率又被称为查准率和查全率，更符合其意义。 \begin{align*}
         & 查准率＝\frac{检索出的相关信息量}{检索出的信息总量}     \\
         & 查全率＝\frac{检索出的相关信息量}{系统中的相关信息总量}
    \end{align*}

    相近的概念还有一个\textbf{准确率（accuracy）}：\[A = \frac{TP+TP}{TP + FN + FP + TN}\]
\end{remark}

\section{满足和优化指标}
% Satisficing and optimizing metrics
要把你顾及到的所有事情组合成单实数评估指标有时并不容易，还有另一种很重要的辅助方法：划分\textbf{优化指标（Optimizing Matric）}和\textbf{满足指标（Satisficing Matric）}。对于优化指标，我们寻求它们的最优值；对于满足指标，只要在一定阈值以内即可。

一个典型例子就是准确率和运行时间，显然运行时间是个满足指标，要求它低于某一阈值即可，而准确率不断优化，寻求其最优值。

\section{训练/开发/测试集划分}
% Train/dev/test distributions
与第二门课的深度学习的实践层面中所述一样。

另外有重要的一点：\textbf{开发集和测试集必须来自同一分布}，开发集确定了你的目标，当你击中目标后，你希望算法能够推广到测试集上，也就是说你树立的目标（开发集）必须与实际的目标（测试集）一致。

\section{开发集和测试集的大小}
% Size of dev and test sets

\section{什么时候该改变开发/测试集和指标？}
% When to change dev/test sets and metric
对于模型的评价标准优势需要根据实际情况进行动态调整，以让模型在实际应用中获得更好的效果。

例如，有时我们不太能接受某些分类错误，于是改变单纯用错误率作为评价标准，给某些分类错误更高的权重，以从追求最小错误率转为追求最小风险。

如果模型在者应用时表现不佳，除了指标设置有误外，还可能是开发/测试集不符合实际应用，这也算是修正评估指标。

总而言之，必须先确定一个评估指标，然后瞄准目标、优化模型，训练、验证、测试，再这个过程中如果有问题（测试时），再修正评估指标，最后在应用中测试（相当于测试集）。虽然无法定义出一个很完美评估指标和开发/测试集，但先设置再修改能加快迭代速度。

\section{为什么是人的表现？}
% Why human-level performance?
为什么要比较机器学习系统和人类的表现，一是人类的表现很多场合接近\textbf{贝叶斯最优误差（Bayes Optimal Error）}，即最优映射；另一方面是随着机器学习的发展，机器学习已经能接近甚至超过人的表现了。

在机器学习中，当模型超过人的表现水平后，它的进步速度常常变得缓慢，这一方面是因为人类的表现接近贝叶斯最优误差，模型的优化空间很少了，另一是因为在模型表现比人类的表现差时，能利用人类的表现来优化模型，比如人为标注数据、人工分析偏差、方差分析，而一旦超过人类水平，这些手段就没什么用了。

\section{可避免偏差}
% Avoidable bias
通过与贝叶斯最优误差，或者说，与人类表现水平的比较，可以表明一个机器学习模型表现的好坏程度，由此判断后续操作应该注重于减小偏差还是减小方差。

模型在训练集上的误差与人类表现水平的差值被称作\textbf{可避免偏差（Avoidable Bias）}。可避免偏差低便意味着模型在训练集上的表现很好，而训练集与验证集之间错误率的差值越小，意味着模型在验证集与测试集上的表现和训练集同样好。

如果可避免偏差大于训练集与验证集之间错误率的差值，之后的工作就应该专注于减小偏差；反之，就应该专注于减小方差。

\section{理解人的表现}
% Understanding human-level performance
我们一般用人类水平误差（Human-level Error）来代表贝叶斯最优误差（或者简称贝叶斯误差）。对于不同领域的例子，不同人群由于其经验水平不一，错误率也不同。一般来说，我们将表现最好的作为人类水平误差。但是实际应用中，不同人选择人类水平误差的基准是不同的，这会带来一定的影响。

例如，如果某模型在训练集上的错误率为 0.7\%，验证集的错误率为 0.8\%。如果选择的人类水平误差为 0.5\%，那么偏差（bias）比方差（variance）更加突出；而如果选择的人类水平误差为 0.7\%，则方差更加突出。也就是说，根据人类水平误差的不同选择，我们可能因此选择不同的优化操作。

\section{超过人的表现}
% Surpassing human- level performance

\section{改善你的模型的表现}
% Improving your model performance
想让一个监督学习算法达到使用程度，应该做到以下两点：
\begin{enumerate}
    \item 算法对训练集的拟合很好，可以看作可避免偏差很低；
    \item 推广到验证集和测试集效果也很好，即方差不是很大。
\end{enumerate}
\begin{figure}[H]
    \centering

    \begin{tikzpicture}
    \node (H) at (0, 0) {Human-level};
    \node (T) at (0, -2) {Training error};
    \node (D) at (0, -4) {Development error};
    \draw[>=Stealth,<->] (H) -- (T);
    \draw[>=Stealth,<->] (T) -- (D);
    \node[right] at ($(H)!0.5!(T) + (0.3, 0)$){\small \schema
        {\schemabox{Avoid bias}}
        {\schemabox{
            使用规模更大的模型\\
            训练更久活使用更好的优化算法\\
            寻找更好的新神经网络架构，或者更好的超参数
        }}
    };
    \node[right] at ($(T)!0.5!(D) + (0.3, 0)$) {\small \schema
        {\schemabox{Variance}}
        {\schemabox{
            收集更多数据\\
            正则化\\
            寻找更好的新神经网络架构，或者更好的超参数
        }}
    };
    \end{tikzpicture}
\end{figure}


\end{document}